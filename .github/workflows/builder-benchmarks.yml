name: Builder Benchmarks

on:
  schedule:
    # Run nightly at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      fixture:
        description: 'Fixture to benchmark (all, small-app, medium-app, large-app)'
        required: false
        default: 'all'
      iterations:
        description: 'Number of iterations per fixture'
        required: false
        default: '5'

jobs:
  benchmark:
    name: Benchmark on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, macos-latest]
        node-version: [20]

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Bun
        uses: oven-sh/setup-bun@v1
        with:
          bun-version: latest

      - name: Install dependencies
        run: bun install

      - name: Setup benchmark fixtures (codegen)
        run: |
          # Generate GraphQL runtime for each fixture
          bun run soda-gql codegen \
            --schema ./benchmarks/runtime-builder/small-app/schema.graphql \
            --out ./benchmarks/runtime-builder/small-app/graphql-system/index.ts

          bun run soda-gql codegen \
            --schema ./benchmarks/runtime-builder/medium-app/schema.graphql \
            --out ./benchmarks/runtime-builder/medium-app/graphql-system/index.ts

          bun run soda-gql codegen \
            --schema ./benchmarks/runtime-builder/large-app/schema.graphql \
            --out ./benchmarks/runtime-builder/large-app/graphql-system/index.ts

      - name: Run benchmark - small-app
        if: github.event.inputs.fixture == 'all' || github.event.inputs.fixture == 'small-app' || github.event.inputs.fixture == ''
        run: |
          ITERATIONS=${{ github.event.inputs.iterations || '5' }}
          bun run perf:builder --fixture small-app --iterations $ITERATIONS

      - name: Run benchmark - medium-app
        if: github.event.inputs.fixture == 'all' || github.event.inputs.fixture == 'medium-app' || github.event.inputs.fixture == ''
        run: |
          ITERATIONS=${{ github.event.inputs.iterations || '5' }}
          bun run perf:builder --fixture medium-app --iterations $ITERATIONS

      - name: Run benchmark - large-app
        if: github.event.inputs.fixture == 'all' || github.event.inputs.fixture == 'large-app' || github.event.inputs.fixture == ''
        run: |
          ITERATIONS=${{ github.event.inputs.iterations || '5' }}
          bun run perf:builder --fixture large-app --iterations $ITERATIONS

      - name: Collect benchmark results
        id: collect-results
        run: |
          # Find latest benchmark results
          LATEST_DIR=$(ls -td .cache/perf/* | head -1)
          echo "results_dir=$LATEST_DIR" >> $GITHUB_OUTPUT

          # Create summary report
          mkdir -p .cache/benchmark-reports
          REPORT_FILE=".cache/benchmark-reports/summary-${{ matrix.os }}.json"

          cat > "$REPORT_FILE" <<EOF
          {
            "os": "${{ matrix.os }}",
            "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "fixtures": []
          }
          EOF

          # Collect all metrics
          for fixture_dir in $LATEST_DIR/*/; do
            if [ -f "$fixture_dir/metrics.json" ]; then
              cat "$fixture_dir/metrics.json" >> "$REPORT_FILE.tmp"
            fi
          done

          echo "report_file=$REPORT_FILE" >> $GITHUB_OUTPUT

      - name: Check for regressions
        id: regression-check
        run: |
          # Download previous baseline (if exists)
          BASELINE_FILE=".cache/baseline-${{ matrix.os }}.json"

          # For now, create baseline if it doesn't exist
          # In future, download from artifact storage
          if [ ! -f "$BASELINE_FILE" ]; then
            echo "No baseline found, creating new baseline"
            cp "${{ steps.collect-results.outputs.report_file }}" "$BASELINE_FILE"
            echo "regression_detected=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          # Simple regression check (5% threshold)
          # TODO: Implement proper statistical comparison
          REGRESSION_THRESHOLD=1.05

          # Compare wall times
          # This is a placeholder - real implementation would parse JSON and compare
          echo "regression_detected=false" >> $GITHUB_OUTPUT

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ matrix.os }}
          path: |
            .cache/perf/**/metrics.json
            .cache/benchmark-reports/*.json
          retention-days: 30

      - name: Comment regression warning
        if: steps.regression-check.outputs.regression_detected == 'true' && github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: '⚠️ **Performance Regression Detected**\n\nBuilder benchmarks show >5% regression on ${{ matrix.os }}. Please review the changes.'
            })

      - name: Notify Slack on regression (scheduled only)
        if: steps.regression-check.outputs.regression_detected == 'true' && github.event_name == 'schedule'
        run: |
          # Placeholder for Slack notification
          # Implement using existing scripts/common.sh hooks or Slack webhook
          echo "Regression detected on ${{ matrix.os }}"
          # curl -X POST -H 'Content-type: application/json' \
          #   --data '{"text":"Builder benchmark regression detected on ${{ matrix.os }}"}' \
          #   ${{ secrets.SLACK_WEBHOOK_URL }}

  aggregate-results:
    name: Aggregate Results
    runs-on: ubuntu-latest
    needs: benchmark
    if: always()

    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: benchmark-artifacts

      - name: Aggregate results
        run: |
          mkdir -p aggregated-results

          # Combine all metrics
          find benchmark-artifacts -name "metrics.json" -exec cat {} \; > aggregated-results/all-metrics.jsonl

          # Create summary
          echo "# Benchmark Results Summary" > aggregated-results/summary.md
          echo "" >> aggregated-results/summary.md
          echo "**Run Date:** $(date -u +%Y-%m-%d)" >> aggregated-results/summary.md
          echo "" >> aggregated-results/summary.md

          # TODO: Parse metrics and generate formatted summary table

      - name: Upload aggregated results
        uses: actions/upload-artifact@v4
        with:
          name: aggregated-benchmark-results
          path: aggregated-results/
          retention-days: 90
